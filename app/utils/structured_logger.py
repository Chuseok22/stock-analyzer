"""
êµ¬ì¡°í™”ëœ ë¡œê·¸ ì‹œìŠ¤í…œ
- ì—°/ì›”/ì¼ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ë¡œê·¸ ì €ì¥
- ë°°í¬ í™˜ê²½ ë³¼ë¥¨ ë§¤í•‘ ì§€ì›
- ë‹¤ì–‘í•œ ë¡œê·¸ ë ˆë²¨ ì§€ì›
- ìœ ì§€ë³´ìˆ˜ ì¹œí™”ì  ì„¤ê³„
"""
import logging
from logging.handlers import TimedRotatingFileHandler
from pathlib import Path
import os
from datetime import datetime, date
from typing import Optional
import json


class StructuredLogger:
    """êµ¬ì¡°í™”ëœ ë¡œê·¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, logger_name: str = "stock_analyzer"):
        self.logger_name = logger_name
        self.logger = None
        
        # Docker í™˜ê²½ê³¼ ë¡œì»¬ í™˜ê²½ì— ë”°ë¥¸ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        if Path("/app").exists():  # Docker í™˜ê²½
            self.base_volume_path = Path("/app")
            self.is_production = True
            print(f"âœ… Docker í™˜ê²½: /app ë¡œê·¸ ì‚¬ìš©")
        elif Path("/volume1/project/stock-analyzer").exists():  # Synology ì§ì ‘ í™˜ê²½
            self.base_volume_path = Path("/volume1/project/stock-analyzer")
            self.is_production = True
            print(f"âœ… ë°°í¬ í™˜ê²½: ë³¼ë¥¨ ë§¤í•‘ ë¡œê·¸ ì‚¬ìš© - {self.base_volume_path}")
        else:  # ë¡œì»¬ ê°œë°œ í™˜ê²½
            self.base_volume_path = Path("storage")
            self.is_production = False
            print("âš ï¸ ê°œë°œ í™˜ê²½: ë¡œì»¬ storage ë¡œê·¸ ì‚¬ìš©")
        
        # ë¡œê·¸ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬
        self.logs_base = self.base_volume_path / "logs"
        self.logs_base.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê±° ì´ˆê¸°í™”
        self._setup_logger()
    
    def _get_log_path(self, log_date: date = None) -> Path:
        """ì—°/ì›”/ì¼ êµ¬ì¡°ë¡œ ë¡œê·¸ ê²½ë¡œ ìƒì„±"""
        if log_date is None:
            log_date = date.today()
        
        year = log_date.year
        month = f"{log_date.month:02d}"
        day = f"{log_date.day:02d}"
        
        # ê²½ë¡œ êµ¬ì¡°: /logs/2025/01/15/
        log_dir = self.logs_base / str(year) / month / day
        log_dir.mkdir(parents=True, exist_ok=True)
        
        return log_dir
    
    def _setup_logger(self):
        """ë¡œê±° ì„¤ì •"""
        self.logger = logging.getLogger(self.logger_name)
        self.logger.setLevel(logging.DEBUG)
        
        # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # í¬ë§·í„° ì„¤ì •
        formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(module)-15s | %(funcName)-20s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬ (ê°œë°œ í™˜ê²½ì—ì„œë§Œ)
        if not self.is_production:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ë“¤ ì„¤ì •
        self._setup_file_handlers(formatter)
    
    def _setup_file_handlers(self, formatter):
        """íŒŒì¼ í•¸ë“¤ëŸ¬ë“¤ ì„¤ì •"""
        today = date.today()
        log_dir = self._get_log_path(today)
        
        # 1. ì „ì²´ ë¡œê·¸ (DEBUG ë ˆë²¨)
        all_log_path = log_dir / f"all_{today.strftime('%Y%m%d')}.log"
        all_handler = logging.FileHandler(all_log_path, encoding='utf-8')
        all_handler.setLevel(logging.DEBUG)
        all_handler.setFormatter(formatter)
        self.logger.addHandler(all_handler)
        
        # 2. ì—ëŸ¬ ë¡œê·¸ë§Œ (ERROR ë ˆë²¨)
        error_log_path = log_dir / f"error_{today.strftime('%Y%m%d')}.log"
        error_handler = logging.FileHandler(error_log_path, encoding='utf-8')
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(formatter)
        self.logger.addHandler(error_handler)
        
        # 3. ì‹¤ì‹œê°„ í•™ìŠµ ì „ìš© ë¡œê·¸
        ml_log_path = log_dir / f"realtime_learning_{today.strftime('%Y%m%d')}.log"
        ml_handler = logging.FileHandler(ml_log_path, encoding='utf-8')
        ml_handler.setLevel(logging.INFO)
        ml_handler.setFormatter(formatter)
        
        # ì‹¤ì‹œê°„ í•™ìŠµ ê´€ë ¨ ë¡œê·¸ë§Œ í•„í„°ë§
        ml_filter = logging.Filter()
        ml_filter.filter = lambda record: 'learning' in record.module.lower() or 'ml' in record.module.lower()
        ml_handler.addFilter(ml_filter)
        self.logger.addHandler(ml_handler)
        
        # 4. ì„±ëŠ¥ ë¡œê·¸ (JSON í˜•íƒœ)
        self.performance_log_path = log_dir / f"performance_{today.strftime('%Y%m%d')}.jsonl"
    
    def log_performance(self, data: dict):
        """ì„±ëŠ¥ ë°ì´í„° JSON ë¡œê·¸"""
        try:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "date": date.today().isoformat(),
                **data
            }
            
            with open(self.performance_log_path, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
                
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def debug(self, message: str):
        """ë””ë²„ê·¸ ë¡œê·¸"""
        self.logger.debug(message)
    
    def info(self, message: str):
        """ì •ë³´ ë¡œê·¸"""
        self.logger.info(message)
    
    def warning(self, message: str):
        """ê²½ê³  ë¡œê·¸"""
        self.logger.warning(message)
    
    def error(self, message: str):
        """ì—ëŸ¬ ë¡œê·¸"""
        self.logger.error(message)
    
    def critical(self, message: str):
        """ì‹¬ê°í•œ ì—ëŸ¬ ë¡œê·¸"""
        self.logger.critical(message)
    
    def log_system_status(self, status: dict):
        """ì‹œìŠ¤í…œ ìƒíƒœ ë¡œê·¸ (êµ¬ì¡°í™”ëœ í˜•íƒœ)"""
        status_data = {
            "type": "system_status",
            "status": status
        }
        self.log_performance(status_data)
        self.info(f"ì‹œìŠ¤í…œ ìƒíƒœ: {status}")
    
    def log_prediction_result(self, market: str, predictions: list, accuracy: float = None):
        """ì˜ˆì¸¡ ê²°ê³¼ ë¡œê·¸"""
        prediction_data = {
            "type": "prediction_result",
            "market": market,
            "prediction_count": len(predictions),
            "accuracy": accuracy,
            "predictions": predictions[:5]  # ìƒìœ„ 5ê°œë§Œ ì €ì¥
        }
        self.log_performance(prediction_data)
        self.info(f"{market} ì‹œì¥ ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì¢…ëª©")
    
    def log_learning_result(self, market: str, old_accuracy: float, new_accuracy: float, strategy: str):
        """í•™ìŠµ ê²°ê³¼ ë¡œê·¸"""
        learning_data = {
            "type": "learning_result",
            "market": market,
            "old_accuracy": old_accuracy,
            "new_accuracy": new_accuracy,
            "improvement": new_accuracy - old_accuracy,
            "strategy": strategy
        }
        self.log_performance(learning_data)
        
        if new_accuracy > old_accuracy:
            self.info(f"{market} í•™ìŠµ ì„±ê³µ: {old_accuracy:.1f}% â†’ {new_accuracy:.1f}% (+{new_accuracy-old_accuracy:.1f}%)")
        else:
            self.warning(f"{market} í•™ìŠµ í›„ ì„±ëŠ¥ ë³€í™”: {old_accuracy:.1f}% â†’ {new_accuracy:.1f}% ({new_accuracy-old_accuracy:.1f}%)")
    
    def create_daily_summary(self, target_date: date = None):
        """ì¼ì¼ ë¡œê·¸ ìš”ì•½ ìƒì„±"""
        if target_date is None:
            target_date = date.today()
        
        try:
            log_dir = self._get_log_path(target_date)
            performance_file = log_dir / f"performance_{target_date.strftime('%Y%m%d')}.jsonl"
            
            if not performance_file.exists():
                self.warning(f"{target_date} ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼ ì—†ìŒ")
                return
            
            # ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„
            summary_data = {
                "prediction_count": 0,
                "learning_count": 0,
                "system_checks": 0,
                "markets": set(),
                "average_accuracy": [],
                "learning_improvements": []
            }
            
            with open(performance_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        entry_type = entry.get('type', '')
                        
                        if entry_type == 'prediction_result':
                            summary_data["prediction_count"] += 1
                            summary_data["markets"].add(entry.get('market', ''))
                            if entry.get('accuracy'):
                                summary_data["average_accuracy"].append(entry['accuracy'])
                        
                        elif entry_type == 'learning_result':
                            summary_data["learning_count"] += 1
                            improvement = entry.get('improvement', 0)
                            summary_data["learning_improvements"].append(improvement)
                        
                        elif entry_type == 'system_status':
                            summary_data["system_checks"] += 1
                            
                    except json.JSONDecodeError:
                        continue
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
            summary_file = log_dir / f"daily_summary_{target_date.strftime('%Y%m%d')}.md"
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"# ì¼ì¼ ë¡œê·¸ ìš”ì•½ - {target_date.strftime('%Yë…„ %mì›” %dì¼')}\n\n")
                f.write(f"## ğŸ“Š í™œë™ ìš”ì•½\n")
                f.write(f"- ì˜ˆì¸¡ ì‹¤í–‰: {summary_data['prediction_count']}íšŒ\n")
                f.write(f"- í•™ìŠµ ì‹¤í–‰: {summary_data['learning_count']}íšŒ\n")
                f.write(f"- ì‹œìŠ¤í…œ ì²´í¬: {summary_data['system_checks']}íšŒ\n")
                f.write(f"- í™œì„± ì‹œì¥: {', '.join(summary_data['markets'])}\n\n")
                
                if summary_data['average_accuracy']:
                    avg_acc = sum(summary_data['average_accuracy']) / len(summary_data['average_accuracy'])
                    f.write(f"## ğŸ¯ ì„±ëŠ¥ ì§€í‘œ\n")
                    f.write(f"- í‰ê·  ì •í™•ë„: {avg_acc:.1f}%\n")
                    f.write(f"- ìµœê³  ì •í™•ë„: {max(summary_data['average_accuracy']):.1f}%\n")
                    f.write(f"- ìµœì € ì •í™•ë„: {min(summary_data['average_accuracy']):.1f}%\n\n")
                
                if summary_data['learning_improvements']:
                    total_improvement = sum(summary_data['learning_improvements'])
                    f.write(f"## ğŸ“ˆ í•™ìŠµ ì„±ê³¼\n")
                    f.write(f"- ì´ ì •í™•ë„ ê°œì„ : {total_improvement:.1f}%p\n")
                    f.write(f"- í‰ê·  ê°œì„ ìœ¨: {total_improvement/len(summary_data['learning_improvements']):.1f}%p\n")
                    f.write(f"- ì„±ê³µì  í•™ìŠµ: {len([x for x in summary_data['learning_improvements'] if x > 0])}íšŒ\n\n")
                
                f.write(f"---\nìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            
            self.info(f"ì¼ì¼ ë¡œê·¸ ìš”ì•½ ìƒì„± ì™„ë£Œ: {summary_file}")
            
        except Exception as e:
            self.error(f"ì¼ì¼ ë¡œê·¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def cleanup_old_logs(self, keep_days: int = 90):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        try:
            from datetime import timedelta
            cutoff_date = date.today() - timedelta(days=keep_days)
            
            deleted_count = 0
            for year_dir in self.logs_base.iterdir():
                if not year_dir.is_dir() or not year_dir.name.isdigit():
                    continue
                
                year = int(year_dir.name)
                if year < cutoff_date.year:
                    # ì „ì²´ ì—°ë„ ì‚­ì œ
                    import shutil
                    shutil.rmtree(year_dir)
                    deleted_count += 1
                    self.info(f"ì˜¤ë˜ëœ ë¡œê·¸ ì—°ë„ ì‚­ì œ: {year}")
                
                elif year == cutoff_date.year:
                    # í•´ë‹¹ ë…„ë„ ë‚´ ì˜¤ë˜ëœ ì›”/ì¼ ì‚­ì œ
                    for month_dir in year_dir.iterdir():
                        if not month_dir.is_dir():
                            continue
                        
                        month = int(month_dir.name)
                        if month < cutoff_date.month:
                            import shutil
                            shutil.rmtree(month_dir)
                            deleted_count += 1
                            self.info(f"ì˜¤ë˜ëœ ë¡œê·¸ ì›” ì‚­ì œ: {year}/{month}")
            
            if deleted_count > 0:
                self.info(f"ë¡œê·¸ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ë””ë ‰í† ë¦¬ ì‚­ì œ")
            else:
                self.info("ì •ë¦¬í•  ì˜¤ë˜ëœ ë¡œê·¸ ì—†ìŒ")
                
        except Exception as e:
            self.error(f"ë¡œê·¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
_global_logger = None

def get_logger(logger_name: str = "stock_analyzer") -> StructuredLogger:
    """ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_logger
    if _global_logger is None:
        _global_logger = StructuredLogger(logger_name)
    return _global_logger


def test_logging_system():
    """ë¡œê·¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë¡œê·¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    logger = get_logger("test_logger")
    
    # ê¸°ë³¸ ë¡œê·¸ í…ŒìŠ¤íŠ¸
    logger.debug("ë””ë²„ê·¸ ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸")
    logger.info("ì •ë³´ ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸")
    logger.warning("ê²½ê³  ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸")
    logger.error("ì—ëŸ¬ ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸")
    
    # êµ¬ì¡°í™”ëœ ë¡œê·¸ í…ŒìŠ¤íŠ¸
    logger.log_system_status({
        "database": "connected",
        "redis": "connected",
        "apis": {"kis": "ok", "alpha_vantage": "ok"}
    })
    
    logger.log_prediction_result("KR", [
        {"stock_code": "005930", "prediction": 2.5},
        {"stock_code": "000660", "prediction": 1.8}
    ], accuracy=72.3)
    
    logger.log_learning_result("KR", 70.1, 72.3, "fine_tune")
    
    # ì¼ì¼ ìš”ì•½ ìƒì„±
    logger.create_daily_summary()
    
    print("âœ… ë¡œê·¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # ìƒì„±ëœ ë¡œê·¸ íŒŒì¼ í™•ì¸
    today = date.today()
    log_dir = logger._get_log_path(today)
    print(f"ğŸ“ ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜: {log_dir}")
    
    for log_file in log_dir.iterdir():
        if log_file.is_file():
            print(f"   ğŸ“„ {log_file.name} ({log_file.stat().st_size} bytes)")


if __name__ == "__main__":
    test_logging_system()
