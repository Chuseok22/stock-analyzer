#!/usr/bin/env python3
"""
ì„±ëŠ¥ ìµœì í™” ì„œë¹„ìŠ¤
- ë¹„ë™ê¸° ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
- ë‹¤ì¸µ ìºì‹± ì‹œìŠ¤í…œ
- DB ì¿¼ë¦¬ ìµœì í™”
- ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ë¦¬ì†ŒìŠ¤ ìµœì í™”
- ë°°ì¹˜ ì²˜ë¦¬ ë° ë³‘ë ¬ ì²˜ë¦¬
"""
import sys
from pathlib import Path
from datetime import datetime, date, timedelta
from typing import List, Dict, Any, Optional, Tuple, Callable
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# ì„ íƒì  ì˜ì¡´ì„±
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False
    print("âš ï¸ aiohttp ë¯¸ì„¤ì¹˜ - ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©")

try:
    import aioredis
    AIOREDIS_AVAILABLE = True
except (ImportError, Exception) as e:
    AIOREDIS_AVAILABLE = False
    print(f"âš ï¸ aioredis í˜¸í™˜ì„± ë¬¸ì œ - ê¸°ë³¸ redis í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©: {e}")
import functools
import time
import gc
from dataclasses import dataclass
import json
import pickle
import hashlib

# Add app directory to path
sys.path.append(str(Path(__file__).parent.parent.parent / "app"))

from app.database.connection import get_db_session
from app.models.entities import StockMaster, StockDailyPrice, MarketRegion
from app.database.redis_client import redis_client
from app.utils.structured_logger import StructuredLogger


@dataclass
class CacheConfig:
    """ìºì‹œ ì„¤ì •"""
    ttl: int = 300              # ê¸°ë³¸ TTL (5ë¶„)
    max_size: int = 1000        # ìµœëŒ€ ìºì‹œ í¬ê¸°
    compression: bool = True    # ì••ì¶• ì‚¬ìš©
    serialization: str = "json" # ì§ë ¬í™” ë°©ì‹ (json, pickle)


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ"""
    execution_time: float
    cache_hit_rate: float
    memory_usage_mb: float
    db_query_count: int
    api_call_count: int
    error_count: int


class AsyncCache:
    """ë¹„ë™ê¸° ìºì‹œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, redis_client, config: CacheConfig = None):
        self.redis = redis_client
        self.config = config or CacheConfig()
        self.local_cache = {}  # L1 ìºì‹œ (ë©”ëª¨ë¦¬)
        self.cache_stats = {'hits': 0, 'misses': 0}
        
    async def get(self, key: str, default=None) -> Any:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ (L1 -> L2 ìˆœì„œ)"""
        try:
            # L1 ìºì‹œ (ë©”ëª¨ë¦¬) í™•ì¸
            if key in self.local_cache:
                entry = self.local_cache[key]
                if entry['expires'] > time.time():
                    self.cache_stats['hits'] += 1
                    return entry['value']
                else:
                    del self.local_cache[key]
            
            # L2 ìºì‹œ (Redis) í™•ì¸
            cached_data = self.redis.get(key)
            if cached_data:
                if self.config.serialization == "pickle":
                    value = pickle.loads(cached_data)
                else:
                    value = json.loads(cached_data)
                
                # L1 ìºì‹œì—ë„ ì €ì¥
                self.local_cache[key] = {
                    'value': value,
                    'expires': time.time() + min(self.config.ttl, 60)  # L1ì€ ìµœëŒ€ 1ë¶„
                }
                
                self.cache_stats['hits'] += 1
                return value
            
            self.cache_stats['misses'] += 1
            return default
            
        except Exception as e:
            print(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨ {key}: {e}")
            return default
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """ìºì‹œì— ê°’ ì €ì¥"""
        try:
            cache_ttl = ttl or self.config.ttl
            
            # ì§ë ¬í™”
            if self.config.serialization == "pickle":
                serialized_value = pickle.dumps(value)
            else:
                serialized_value = json.dumps(value, default=str)
            
            # L2 ìºì‹œ (Redis) ì €ì¥
            success = self.redis.set(key, serialized_value, cache_ttl)
            
            # L1 ìºì‹œ (ë©”ëª¨ë¦¬) ì €ì¥
            if len(self.local_cache) < self.config.max_size:
                self.local_cache[key] = {
                    'value': value,
                    'expires': time.time() + min(cache_ttl, 60)
                }
            
            return success
            
        except Exception as e:
            print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {key}: {e}")
            return False
    
    def get_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ë°˜í™˜"""
        total = self.cache_stats['hits'] + self.cache_stats['misses']
        return self.cache_stats['hits'] / total if total > 0 else 0.0
    
    def clear_local_cache(self):
        """L1 ìºì‹œ ì •ë¦¬"""
        current_time = time.time()
        expired_keys = [k for k, v in self.local_cache.items() if v['expires'] <= current_time]
        for key in expired_keys:
            del self.local_cache[key]


def cache_result(ttl: int = 300, key_prefix: str = ""):
    """ê²°ê³¼ ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = f"{key_prefix}:{func.__name__}:{hashlib.md5(str(args + tuple(kwargs.items())).encode()).hexdigest()}"
            
            # ìºì‹œì—ì„œ ì¡°íšŒ
            cache = AsyncCache(redis_client)
            cached_result = await cache.get(cache_key)
            
            if cached_result is not None:
                return cached_result
            
            # ìºì‹œ ë¯¸ìŠ¤ - í•¨ìˆ˜ ì‹¤í–‰
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            # ê²°ê³¼ ìºì‹±
            await cache.set(cache_key, result, ttl)
            return result
        
        return wrapper
    return decorator


class AsyncAPIClient:
    """ë¹„ë™ê¸° API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, max_concurrent: int = 10, rate_limit: float = 0.1):
        self.max_concurrent = max_concurrent
        self.rate_limit = rate_limit
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.last_request_time = 0
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=self.max_concurrent)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def request(self, method: str, url: str, **kwargs) -> Dict[str, Any]:
        """ë¹„ë™ê¸° HTTP ìš”ì²­"""
        async with self.semaphore:
            # Rate limiting
            current_time = time.time()
            time_since_last = current_time - self.last_request_time
            if time_since_last < self.rate_limit:
                await asyncio.sleep(self.rate_limit - time_since_last)
            
            self.last_request_time = time.time()
            
            try:
                async with self.session.request(method, url, **kwargs) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        print(f"API ìš”ì²­ ì‹¤íŒ¨ {url}: {response.status}")
                        return {}
            except Exception as e:
                print(f"API ìš”ì²­ ì˜¤ë¥˜ {url}: {e}")
                return {}


class BatchProcessor:
    """ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, batch_size: int = 100, max_workers: int = 4):
        self.batch_size = batch_size
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
    async def process_in_batches(self, items: List[Any], processor: Callable, 
                               *args, **kwargs) -> List[Any]:
        """ì•„ì´í…œë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ì²˜ë¦¬"""
        results = []
        
        # ë°°ì¹˜ë¡œ ë¶„í• 
        batches = [items[i:i + self.batch_size] for i in range(0, len(items), self.batch_size)]
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        for batch in batches:
            if asyncio.iscoroutinefunction(processor):
                task = processor(batch, *args, **kwargs)
            else:
                # CPU ì§‘ì•½ì  ì‘ì—…ì€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                task = loop.run_in_executor(self.executor, processor, batch, *args, **kwargs)
            tasks.append(task)
        
        # ëª¨ë“  ë°°ì¹˜ ì™„ë£Œ ëŒ€ê¸°
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ì·¨í•©
        for batch_result in batch_results:
            if isinstance(batch_result, Exception):
                print(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {batch_result}")
                continue
            if isinstance(batch_result, list):
                results.extend(batch_result)
            else:
                results.append(batch_result)
        
        return results
    
    def __del__(self):
        self.executor.shutdown(wait=False)


class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”"""
    
    @staticmethod
    def bulk_insert_optimized(session, model_class, data_list: List[Dict], 
                            batch_size: int = 1000):
        """ìµœì í™”ëœ ëŒ€ëŸ‰ ì‚½ì…"""
        try:
            for i in range(0, len(data_list), batch_size):
                batch = data_list[i:i + batch_size]
                session.bulk_insert_mappings(model_class, batch)
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬
                if i % (batch_size * 5) == 0:
                    session.flush()
                    gc.collect()
            
            session.commit()
            return True
            
        except Exception as e:
            session.rollback()
            print(f"ëŒ€ëŸ‰ ì‚½ì… ì‹¤íŒ¨: {e}")
            return False
    
    @staticmethod
    def get_optimized_query_builder():
        """ìµœì í™”ëœ ì¿¼ë¦¬ ë¹Œë”"""
        return {
            'select_related': True,      # ê´€ë ¨ ê°ì²´ ë¯¸ë¦¬ ë¡œë“œ
            'batch_size': 1000,          # ë°°ì¹˜ í¬ê¸°
            'use_index': True,           # ì¸ë±ìŠ¤ íŒíŠ¸ ì‚¬ìš©
            'read_only': True            # ì½ê¸° ì „ìš© ìµœì í™”
        }


class PerformanceOptimizer:
    """ì„±ëŠ¥ ìµœì í™” ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = StructuredLogger("performance_optimizer")
        self.cache = AsyncCache(redis_client)
        self.batch_processor = BatchProcessor()
        self.api_client = None
        self.metrics = PerformanceMetrics(
            execution_time=0.0,
            cache_hit_rate=0.0,
            memory_usage_mb=0.0,
            db_query_count=0,
            api_call_count=0,
            error_count=0
        )
        
        self.logger.info("ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    
    @cache_result(ttl=300, key_prefix="stock_data")
    async def get_stock_data_cached(self, stock_codes: List[str], 
                                  region: MarketRegion) -> List[Dict]:
        """ìºì‹œëœ ì£¼ì‹ ë°ì´í„° ì¡°íšŒ"""
        self.logger.info(f"ìºì‹œëœ ì£¼ì‹ ë°ì´í„° ì¡°íšŒ: {len(stock_codes)}ê°œ ì¢…ëª©")
        
        try:
            start_time = time.time()
            
            with get_db_session() as db:
                # ìµœì í™”ëœ ì¿¼ë¦¬
                stocks = db.query(StockMaster).filter(
                    StockMaster.market_region == region.value,
                    StockMaster.stock_code.in_(stock_codes),
                    StockMaster.is_active == True
                ).all()
                
                self.metrics.db_query_count += 1
                
                # ìµœê·¼ 30ì¼ ê°€ê²© ë°ì´í„°
                end_date = datetime.now().date()
                start_date = end_date - timedelta(days=30)
                
                stock_data = []
                for stock in stocks:
                    prices = db.query(StockDailyPrice).filter(
                        StockDailyPrice.stock_id == stock.stock_id,
                        StockDailyPrice.trade_date >= start_date,
                        StockDailyPrice.trade_date <= end_date
                    ).order_by(StockDailyPrice.trade_date.desc()).limit(30).all()
                    
                    if prices:
                        stock_info = {
                            'stock_code': stock.stock_code,
                            'stock_name': stock.stock_name,
                            'market_region': stock.market_region,
                            'current_price': float(prices[0].close_price),
                            'price_change_pct': float(prices[0].daily_return_pct) if prices[0].daily_return_pct else 0,
                            'volume': int(prices[0].volume) if prices[0].volume else 0,
                            'prices': [{
                                'date': p.trade_date.isoformat(),
                                'close': float(p.close_price),
                                'volume': int(p.volume) if p.volume else 0
                            } for p in prices[:10]]  # ìµœê·¼ 10ì¼ë§Œ
                        }
                        stock_data.append(stock_info)
                
                self.metrics.execution_time = time.time() - start_time
                self.logger.info(f"ì£¼ì‹ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(stock_data)}ê°œ, {self.metrics.execution_time:.2f}ì´ˆ")
                
                return stock_data
                
        except Exception as e:
            self.metrics.error_count += 1
            self.logger.error(f"ìºì‹œëœ ì£¼ì‹ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def parallel_api_calls(self, urls: List[str], headers: Dict = None) -> List[Dict]:
        """ë³‘ë ¬ API í˜¸ì¶œ"""
        self.logger.info(f"ë³‘ë ¬ API í˜¸ì¶œ: {len(urls)}ê°œ URL")
        
        try:
            async with AsyncAPIClient(max_concurrent=5, rate_limit=0.2) as client:
                tasks = []
                for url in urls:
                    task = client.request('GET', url, headers=headers or {})
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
                valid_results = []
                for result in results:
                    if isinstance(result, Exception):
                        self.metrics.error_count += 1
                    elif isinstance(result, dict) and result:
                        valid_results.append(result)
                        self.metrics.api_call_count += 1
                
                self.logger.info(f"ë³‘ë ¬ API í˜¸ì¶œ ì™„ë£Œ: {len(valid_results)}/{len(urls)}ê°œ ì„±ê³µ")
                return valid_results
                
        except Exception as e:
            self.logger.error(f"ë³‘ë ¬ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def optimized_data_collection(self, stock_codes: List[str], 
                                      region: MarketRegion) -> List[Dict]:
        """ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘"""
        self.logger.info(f"ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘: {region.value} {len(stock_codes)}ê°œ ì¢…ëª©")
        
        try:
            start_time = time.time()
            
            # 1. ìºì‹œëœ ë°ì´í„° ìš°ì„  ì¡°íšŒ
            cached_data = await self.get_stock_data_cached(stock_codes, region)
            
            # 2. ìºì‹œ ë¯¸ìŠ¤ ì¢…ëª©ë“¤ì— ëŒ€í•´ ë°°ì¹˜ ì²˜ë¦¬
            cached_codes = {item['stock_code'] for item in cached_data}
            missing_codes = [code for code in stock_codes if code not in cached_codes]
            
            if missing_codes:
                self.logger.info(f"ìºì‹œ ë¯¸ìŠ¤ ì¢…ëª©: {len(missing_codes)}ê°œ")
                
                # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëˆ„ë½ëœ ë°ì´í„° ìˆ˜ì§‘
                missing_data = await self.batch_processor.process_in_batches(
                    missing_codes,
                    self._collect_single_stock_data,
                    region
                )
                
                # ê²°ê³¼ ë³‘í•©
                cached_data.extend([item for item in missing_data if item])
            
            # 3. ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
            self.metrics.execution_time = time.time() - start_time
            self.metrics.cache_hit_rate = len(cached_codes) / len(stock_codes) if stock_codes else 0
            
            self.logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(cached_data)}ê°œ, ìºì‹œ íˆíŠ¸ìœ¨: {self.metrics.cache_hit_rate:.1%}")
            
            return cached_data
            
        except Exception as e:
            self.logger.error(f"ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _collect_single_stock_data(self, stock_codes: List[str], 
                                       region: MarketRegion) -> List[Dict]:
        """ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬ìš©)"""
        results = []
        
        try:
            with get_db_session() as db:
                for code in stock_codes:
                    stock = db.query(StockMaster).filter_by(
                        market_region=region.value,
                        stock_code=code,
                        is_active=True
                    ).first()
                    
                    if not stock:
                        continue
                    
                    # ìµœê·¼ ë°ì´í„° ì¡°íšŒ
                    recent_price = db.query(StockDailyPrice).filter_by(
                        stock_id=stock.stock_id
                    ).order_by(StockDailyPrice.trade_date.desc()).first()
                    
                    if recent_price:
                        stock_info = {
                            'stock_code': stock.stock_code,
                            'stock_name': stock.stock_name,
                            'market_region': stock.market_region,
                            'current_price': float(recent_price.close_price),
                            'price_change_pct': float(recent_price.daily_return_pct) if recent_price.daily_return_pct else 0,
                            'volume': int(recent_price.volume) if recent_price.volume else 0
                        }
                        results.append(stock_info)
            
            return results
            
        except Exception as e:
            self.logger.error(f"ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def optimize_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        try:
            # L1 ìºì‹œ ì •ë¦¬
            self.cache.clear_local_cache()
            
            # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            collected = gc.collect()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.metrics.memory_usage_mb = memory_mb
            
            self.logger.info(f"ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {collected}ê°œ ê°ì²´ ì •ë¦¬, í˜„ì¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")
            
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
    
    async def batch_update_database(self, updates: List[Dict], 
                                  model_class, batch_size: int = 1000) -> bool:
        """ë°°ì¹˜ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        self.logger.info(f"ë°°ì¹˜ DB ì—…ë°ì´íŠ¸: {len(updates)}ê°œ ë ˆì½”ë“œ")
        
        try:
            with get_db_session() as db:
                success = DatabaseOptimizer.bulk_insert_optimized(
                    db, model_class, updates, batch_size
                )
                
                if success:
                    self.metrics.db_query_count += len(updates) // batch_size + 1
                    self.logger.info("ë°°ì¹˜ DB ì—…ë°ì´íŠ¸ ì„±ê³µ")
                else:
                    self.metrics.error_count += 1
                    self.logger.error("ë°°ì¹˜ DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                
                return success
                
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        return {
            'execution_time': f"{self.metrics.execution_time:.2f}ì´ˆ",
            'cache_hit_rate': f"{self.metrics.cache_hit_rate:.1%}",
            'memory_usage': f"{self.metrics.memory_usage_mb:.1f}MB",
            'db_queries': self.metrics.db_query_count,
            'api_calls': self.metrics.api_call_count,
            'errors': self.metrics.error_count,
            'cache_stats': {
                'hits': self.cache.cache_stats['hits'],
                'misses': self.cache.cache_stats['misses'],
                'hit_rate': f"{self.cache.get_hit_rate():.1%}"
            }
        }
    
    async def run_performance_benchmark(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        self.logger.info("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        try:
            benchmark_results = {}
            
            # 1. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            start_time = time.time()
            test_data = {'test': 'data', 'timestamp': datetime.now().isoformat()}
            
            for i in range(100):
                await self.cache.set(f"benchmark_key_{i}", test_data)
                retrieved = await self.cache.get(f"benchmark_key_{i}")
            
            cache_time = time.time() - start_time
            benchmark_results['cache_performance'] = f"{cache_time:.3f}ì´ˆ"
            
            # 2. DB ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            start_time = time.time()
            
            with get_db_session() as db:
                stocks = db.query(StockMaster).filter_by(is_active=True).limit(100).all()
            
            db_time = time.time() - start_time
            benchmark_results['db_query_performance'] = f"{db_time:.3f}ì´ˆ"
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            benchmark_results['memory_usage'] = f"{memory_mb:.1f}MB"
            
            # 4. ì „ì²´ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            performance_score = 100 - min(cache_time * 10 + db_time * 5 + memory_mb * 0.1, 90)
            benchmark_results['performance_score'] = f"{performance_score:.1f}/100"
            
            self.logger.info(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ: {performance_score:.1f}ì ")
            
            return benchmark_results
            
        except Exception as e:
            self.logger.error(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}


# ê¸€ë¡œë²Œ ì„±ëŠ¥ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
performance_optimizer = PerformanceOptimizer()


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("âš¡ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    optimizer = PerformanceOptimizer()
    
    try:
        # 1. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        print("\n1ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        benchmark = await optimizer.run_performance_benchmark()
        
        for key, value in benchmark.items():
            print(f"   {key}: {value}")
        
        # 2. ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
        print("\n2ï¸âƒ£ ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
        test_codes = ['005930', '000660', '035420', 'AAPL', 'MSFT']
        
        # í•œêµ­ ì¢…ëª©
        kr_data = await optimizer.optimized_data_collection(
            test_codes[:3], MarketRegion.KR
        )
        print(f"   í•œêµ­ ë°ì´í„°: {len(kr_data)}ê°œ ìˆ˜ì§‘")
        
        # 3. ì„±ëŠ¥ ë¦¬í¬íŠ¸
        print("\n3ï¸âƒ£ ì„±ëŠ¥ ë¦¬í¬íŠ¸")
        report = optimizer.get_performance_report()
        
        for key, value in report.items():
            if isinstance(value, dict):
                print(f"   {key}:")
                for sub_key, sub_value in value.items():
                    print(f"     {sub_key}: {sub_value}")
            else:
                print(f"   {key}: {value}")
        
        # 4. ë©”ëª¨ë¦¬ ìµœì í™”
        print("\n4ï¸âƒ£ ë©”ëª¨ë¦¬ ìµœì í™”")
        optimizer.optimize_memory_usage()
        
        print("\nğŸ‰ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
